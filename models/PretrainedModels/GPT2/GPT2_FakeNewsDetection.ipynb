{"cells":[{"cell_type":"markdown","metadata":{"id":"qb9qKvDdMbNX"},"source":["## Setup Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39611,"status":"ok","timestamp":1690470387428,"user":{"displayName":"Abanoub Abdelmalak","userId":"09340681942671294066"},"user_tz":-180},"id":"Pfi1QUkuEss5","outputId":"0a2c0953-6d5e-4b0f-fb9a-12f6ce2469ad"},"outputs":[],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":0,"status":"ok","timestamp":1690470332446,"user":{"displayName":"Abanoub Abdelmalak","userId":"09340681942671294066"},"user_tz":-180},"id":"jYNPBqRnEdJT","outputId":"ad2b73e0-d03e-4261-f6db-8f0a7195341e"},"outputs":[],"source":["! pip install transformers\n","! pip install pycaret\n","! pip install squarify\n","! pip install nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":309,"status":"ok","timestamp":1690448942266,"user":{"displayName":"Abanoub Abdelmalak","userId":"09340681942671294066"},"user_tz":-180},"id":"HIFgZcfWlC8F","outputId":"8f02170b-52c3-40d5-e429-7c28c8ed5839"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import pycaret\n","import nltk, re, string\n","from string import punctuation\n","from nltk.corpus import stopwords\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast, GPT2Tokenizer, GPT2ForSequenceClassification, AdamW, GPT2LMHeadModel, GPT2TokenizerFast\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import ConfusionMatrixDisplay\n","from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,roc_auc_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import torch\n","import torch.nn as nn\n","# specify GPU\n","device = torch.device(\"cuda\")\n","nltk.download('stopwords')\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":285,"status":"ok","timestamp":1690441215764,"user":{"displayName":"Abanoub Abdelmalak","userId":"09340681942671294066"},"user_tz":-180},"id":"-xkkoUVsEu_w","outputId":"48e2aa9d-d5cf-4fdb-e569-d995e267fe9f"},"outputs":[],"source":["# Set Working Directory\n","# %cd /content/drive/My Drive/Colab Notebooks/DL-Project\n","%cd \"C:\\Abanoub Abdelmalak\\OMSCS\\CS7643_DL\\Group Project\\cs7643-groupproject\\\"\n","# %cd \"/content/drive/MyDrive/OMSCS/CS7643_DL/GroupProject/cs7643-groupproject\""]},{"cell_type":"markdown","metadata":{"id":"Ar3qNZz0MiGk"},"source":["## Load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":223},"executionInfo":{"elapsed":1000,"status":"ok","timestamp":1690449011753,"user":{"displayName":"Abanoub Abdelmalak","userId":"09340681942671294066"},"user_tz":-180},"id":"9Gkv1hk-mr9Z","outputId":"5d885fd2-fb43-40ce-d00f-efcfbd0dcb56"},"outputs":[],"source":["\n","data = pd.read_csv('./Data/final_fake_news.csv', delimiter=';')\n","data[\"Target\"] = data[\"label\"].apply(lambda x: \"Fake\" if x == 0 else \"True\")\n","print(\"Without dummy assignment:\")\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data['label'] = pd.get_dummies(data.Target)['Fake']\n","print(\"\\nAfter dummy assignment:\")\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":428},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690449012099,"user":{"displayName":"Abanoub Abdelmalak","userId":"09340681942671294066"},"user_tz":-180},"id":"6GR5qqqwoVAc","outputId":"ad4e535f-132a-4435-a60f-56fffbe238ea"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import squarify\n","\n","label_size = [data['label'].sum(), len(data['label']) - data['label'].sum()]\n","labels = ['Fake', 'True']\n","colors = ['#98FB98', '#9370DB']\n","\n","fig, ax = plt.subplots()\n","squarify.plot(sizes=label_size, label=labels, color=colors, alpha=0.7, ax=ax)\n","\n","plt.title('Label Distribution')\n","plt.axis('off')\n","\n","for i, rect in enumerate(ax.patches):\n","    percentage = label_size[i] / sum(label_size) * 100\n","    x = rect.get_x() + rect.get_width() / 2\n","    y = rect.get_y() + rect.get_height() / 3\n","    plt.text(x=x, y=y, s=f'{percentage:.1f}%', ha='center', va='center', fontsize=10, color='blue')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690449012909,"user":{"displayName":"Abanoub Abdelmalak","userId":"09340681942671294066"},"user_tz":-180},"id":"MxZi1b0sNHDK","outputId":"b1fd5f71-452b-4ae3-f293-bff96296fc4e"},"outputs":[],"source":["# Check if there are null values\n","if data.isnull().values.any():\n","    print(\"There are null values in the dataset.\")\n","else:\n","    print(\"There are no null values in the dataset.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1690449012910,"user":{"displayName":"Abanoub Abdelmalak","userId":"09340681942671294066"},"user_tz":-180},"id":"hkp5GItGNc0B","outputId":"135f8d44-28d4-46b0-9a5c-3a2ec69c5513"},"outputs":[],"source":["data.head()"]},{"cell_type":"markdown","metadata":{"id":"Zj5h7QbnZC0t"},"source":["## Train-test-split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3P9DYEq3y6pR"},"outputs":[],"source":["train_text, temp_text, train_labels, temp_labels = train_test_split(data['text'], data['label'],\n","                                                                    random_state=2018,\n","                                                                    test_size=0.3,\n","                                                                    stratify=data['Target'])\n","val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n","                                                                random_state=2018,\n","                                                                test_size=0.5,\n","                                                                stratify=temp_labels)"]},{"cell_type":"markdown","metadata":{"id":"2ItKm0hDMujq"},"source":["# Creating a new model\n","\n","The model is a Custom Multiheaded Transformer. The reason behind the architecture is to allow the transformer to work in the sense where each head is responsible for analyzing different segments of the sentence. Then allow the heads are mashed together using softmax function.\n"]},{"cell_type":"markdown","metadata":{"id":"HIkcURDhz2R0"},"source":["### Load pretrained GPT Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1639,"status":"ok","timestamp":1690449015555,"user":{"displayName":"Abanoub Abdelmalak","userId":"09340681942671294066"},"user_tz":-180},"id":"w6skrhdVzW0b","outputId":"bc5ab19f-8a26-4265-8d6e-04d2ccd86f8f"},"outputs":[],"source":["# Load BERT model and tokenizer\n","gpt2 = AutoModel.from_pretrained('gpt2')\n","tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n","# gpt2 = AutoModel.from_pretrained('gpt2-xl')\n","# tokenizer = GPT2TokenizerFast.from_pretrained('gpt2-xl')\n","if tokenizer.pad_token is None:\n","    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","    gpt2.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"markdown","metadata":{"id":"H1G4xtl7dXBD"},"source":["### Prepare Input Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":524},"executionInfo":{"elapsed":795,"status":"ok","timestamp":1690449016617,"user":{"displayName":"Abanoub Abdelmalak","userId":"09340681942671294066"},"user_tz":-180},"id":"FDZx-O2xzcA_","outputId":"1fb40335-20b2-4bd2-eae2-f15fc8947159"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from scipy import stats\n","\n","# Calculate the number of words in each title\n","seq_len = [len(title.split()) for title in train_text]\n","\n","plt.hist(seq_len, bins=40, color='navy')\n","\n","plt.xlabel('Number of Words')\n","plt.ylabel('Number of Texts')\n","plt.title('Histogram of Number of Words in Train Data Titles')\n","\n","# Calculate the median\n","median_value = np.median(seq_len)\n","print(\"Median:\", median_value)\n","\n","# Calculate the mode\n","mode_value = stats.mode(seq_len)\n","print(\"Mode:\", mode_value.mode[0])\n","\n","# Calculate the mean\n","mean_value = np.mean(seq_len)\n","print(\"Mean:\", mean_value)\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rfwINnZozkyd"},"outputs":[],"source":["# Set max title length\n","MAX_LENGHT = 32\n","tokens_train = tokenizer.batch_encode_plus(\n","    train_text.tolist(),\n","    max_length = MAX_LENGHT,\n","    pad_to_max_length=True,\n","    truncation=True\n",")\n","tokens_val = tokenizer.batch_encode_plus(\n","    val_text.tolist(),\n","    max_length = MAX_LENGHT,\n","    pad_to_max_length=True,\n","    truncation=True\n",")\n","tokens_test = tokenizer.batch_encode_plus(\n","    test_text.tolist(),\n","    max_length = MAX_LENGHT,\n","    pad_to_max_length=True,\n","    truncation=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jE9PI9_H0Moi"},"outputs":[],"source":["train_seq = torch.tensor(tokens_train['input_ids'])\n","train_mask = torch.tensor(tokens_train['attention_mask'])\n","train_y = torch.tensor(train_labels.tolist())\n","\n","val_seq = torch.tensor(tokens_val['input_ids'])\n","val_mask = torch.tensor(tokens_val['attention_mask'])\n","val_y = torch.tensor(val_labels.tolist())\n","\n","test_seq = torch.tensor(tokens_test['input_ids'])\n","test_mask = torch.tensor(tokens_test['attention_mask'])\n","test_y = torch.tensor(test_labels.tolist())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oft-16jR0M6h"},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","batch_size = 32\n","\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","val_sampler = SequentialSampler(val_data)\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n","\n","test_data = TensorDataset(test_seq, test_mask, test_y)\n","test_sampler = SequentialSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dH-wI1yhzQkD"},"outputs":[],"source":["# Freeze the parameters of the pre-trained BERT model\n","for param in gpt2.parameters():\n","    param.requires_grad = True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","class TransformerTranslator(nn.Module):\n","    \"\"\"\n","    A single-layer Transformer which encodes a sequence of text and\n","    performs binary classification.\n","\n","    The model has a vocab size of V, works on\n","    sequences of length T, has an hidden dimension of H, uses word vectors\n","    also of dimension H, and operates on minibatches of size N.\n","    \"\"\"\n","\n","    def __init__(self, input_size, output_size, device, hidden_dim=768, num_heads=2, dim_feedforward=2048, dim_k=96, dim_v=96, dim_q=96, max_length=MAX_LENGHT):\n","        \"\"\"\n","        :param input_size: the size of the input, which equals to the number of words in source language vocabulary\n","        :param output_size: the size of the output, which equals to the number of words in target language vocabulary\n","        :param hidden_dim: the dimensionality of the output embeddings that go into the final layer\n","        :param num_heads: the number of Transformer heads to use\n","        :param dim_feedforward: the dimension of the feedforward network model\n","        :param dim_k: the dimensionality of the key vectors\n","        :param dim_q: the dimensionality of the query vectors\n","        :param dim_v: the dimensionality of the value vectors\n","        \"\"\"\n","        super(TransformerTranslator, self).__init__()\n","        assert hidden_dim % num_heads == 0\n","\n","        self.num_heads = num_heads\n","        self.word_embedding_dim = hidden_dim\n","        self.hidden_dim = hidden_dim\n","        self.dim_feedforward = dim_feedforward\n","        self.max_length = max_length\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.device = device\n","        self.dim_k = dim_k\n","        self.dim_v = dim_v\n","        self.dim_q = dim_q\n","\n","        # initialize word embedding layer\n","        self.embeddingL = nn.Embedding(input_size, hidden_dim).to(device)\n","        # initialize positional embedding layer\n","        self.posembeddingL = nn.Embedding(max_length, hidden_dim).to(device)\n","\n","        self.k1 = nn.Linear(self.hidden_dim, self.dim_k)\n","        self.v1 = nn.Linear(self.hidden_dim, self.dim_v)\n","        self.q1 = nn.Linear(self.hidden_dim, self.dim_q)\n","        self.k2 = nn.Linear(self.hidden_dim, self.dim_k)\n","        self.v2 = nn.Linear(self.hidden_dim, self.dim_v)\n","        self.q2 = nn.Linear(self.hidden_dim, self.dim_q)\n","\n","        self.softmax = nn.Softmax(dim=2)\n","        self.attention_head_projection = nn.Linear(\n","            self.dim_v * self.num_heads, self.hidden_dim)\n","        self.norm_mh = nn.LayerNorm(self.hidden_dim)\n","\n","        self.feedforward = nn.Sequential(\n","            nn.Linear(self.hidden_dim, self.dim_feedforward),\n","            nn.ReLU(),\n","            nn.Linear(self.dim_feedforward, self.hidden_dim)).to(device)\n","        self.norm_ff = nn.LayerNorm(self.hidden_dim)\n","\n","        self.output_layer = nn.Linear(hidden_dim, output_size).to(device)\n","        self.dropout = nn.Dropout(0.5)\n","\n","\n","    def forward(self, inputs):\n","        \"\"\"\n","        This function computes the full Transformer forward pass.\n","        Put together all of the layers you've developed in the correct order.\n","\n","        :param inputs: a PyTorch tensor of shape (N,T). These are integer lookups.\n","\n","        :returns: the model outputs. Should be scores of shape (N,T,output_size).\n","        \"\"\"\n","        # embeds = self.embed(inputs)\n","        hidden_states = self.multi_head_attention(inputs)\n","        outputs = self.feedforward_layer(hidden_states)\n","        scores = self.final_layer(self.dropout(outputs))\n","        return scores\n","\n","    def embed(self, inputs):\n","        \"\"\"\n","        :param inputs: intTensor of shape (N,T)\n","        :returns embeddings: floatTensor of shape (N,T,H)\n","        \"\"\"\n","        # Word Embedding\n","        word_emb = self.embeddingL(inputs)\n","        # Positional Encoding\n","        positions = torch.arange(\n","            self.max_length, device=self.device).unsqueeze(0)\n","        pos_encode = self.posembeddingL(positions)\n","        x = pos_encode + word_emb\n","        return x\n","\n","    def multi_head_attention(self, inputs):\n","        \"\"\"\n","        :param inputs: float32 Tensor of shape (N,T,H)\n","        :returns outputs: float32 Tensor of shape (N,T,H)\n","\n","        Traditionally we'd include a padding mask here, so that pads are ignored.\n","        This is a simplified implementation.\n","        \"\"\"\n","        # Multi-head Attention\n","        k1 = self.k1(inputs)\n","        v1 = self.v1(inputs)\n","        q1 = self.q1(inputs)\n","        k2 = self.k2(inputs)\n","        v2 = self.v2(inputs)\n","        q2 = self.q2(inputs)\n","\n","        # print(\"Debug: q1_shape\", q1.shape)\n","        # print(\"Debug: k1_shape\", k1.shape)\n","        # print(\"Debug: q2_shape\", q2.shape)\n","        # print(\"Debug: k2_shape\", k2.shape)\n","        # print(\"Debug: v2_shape\", v2.shape)\n","                \n","        z1 = torch.bmm(q1, k1.transpose(1, 2)) / np.sqrt(self.dim_k)\n","        z1 = torch.softmax(z1, dim=-1)\n","        z11 = torch.bmm(z1, v1)\n","        z2 = torch.bmm(q2, k2.transpose(1, 2)) / np.sqrt(self.dim_k)\n","        z2 = torch.softmax(z2, dim=-1)\n","        z22 = torch.bmm(z2, v2)\n","        z = torch.cat((z11, z22), dim=-1)\n","        z = self.attention_head_projection(z)\n","        z = self.norm_mh(z + inputs)\n","        return z\n","\n","    def feedforward_layer(self, inputs):\n","        \"\"\"\n","        :param inputs: float32 Tensor of shape (N,T,H)\n","        :returns outputs: float32 Tensor of shape (N,T,H)\n","        \"\"\"\n","        outputs = self.feedforward(inputs)\n","        outputs = self.norm_ff(outputs + inputs)\n","        return outputs\n","\n","    def final_layer(self, inputs):\n","        \"\"\"\n","        :param inputs: float32 Tensor of shape (N,T,H)\n","        :returns outputs: float32 Tensor of shape (N,T,V)\n","        \"\"\"\n","        outputs = self.output_layer(inputs)\n","        return outputs\n","\n","#Set hyperparameters\n","vocab = tokenizer.get_vocab()\n","vocab_size = len(vocab)\n","output_size = 2\n","l2norm=0.05"]},{"cell_type":"markdown","metadata":{"id":"XAJPyGH4zaRZ"},"source":["### Define Model Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5oC6f5jD0vm0"},"outputs":[],"source":["class GPT2Pooler(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.dense = nn.Linear(768, 768).to(device)\n","        self.activation = nn.ReLU().to(device)\n","\n","    def forward(self, hidden_states):\n","        first_token_tensor = hidden_states[:, 0]\n","        pooled_output = self.dense(first_token_tensor)\n","        pooled_output = self.activation(pooled_output)\n","        return pooled_output\n","\n","\n","class GPT2Pooler2(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.dense = nn.Linear(2, 2).to(device)\n","        self.activation = nn.ReLU().to(device)\n","\n","    def forward(self, hidden_states):\n","        first_token_tensor = hidden_states[:, 0]\n","        pooled_output = self.dense(first_token_tensor)\n","        pooled_output = self.activation(pooled_output)\n","        return pooled_output\n","\n","\n","class GPT2Model_2FC(nn.Module):\n","    def __init__(self, model, dropout=0.2):\n","      super(GPT2Model_2FC, self).__init__()\n","      self.model = model.to(device)\n","      self.pooler = GPT2Pooler()\n","\n","      self.dropout = nn.Dropout(dropout).to(device)\n","      self.relu =  nn.ReLU().to(device)\n","      self.fc1 = nn.Linear(768,512).to(device)\n","\n","      self.sigmoid = nn.Sigmoid().to(device)\n","      self.fc2 = nn.Linear(512,2).to(device)\n","      self.softmax = nn.LogSoftmax(dim=1).to(device)\n","      self.tanh = nn.Tanh().to(device)\n","      return\n","\n","    def forward(self, sent_id, mask):\n","      output_model = self.model(sent_id, attention_mask=mask)\n","      cls_hs = output_model[\"last_hidden_state\"]\n","      x = self.pooler(cls_hs)\n","      x = self.fc1(x)\n","      x = self.relu(x)\n","      x = self.dropout(x)\n","      x = self.fc2(x)\n","      x = self.softmax(x)\n","      return x\n","\n","\n","class GPT2Model_1FC(nn.Module):\n","    def __init__(self, model, dropout=0.2):\n","      super(GPT2Model_1FC, self).__init__()\n","      self.model = model.to(device)\n","      self.pooler = GPT2Pooler()\n","      self.dropout = nn.Dropout(dropout).to(device)\n","      self.relu =  nn.ReLU().to(device)\n","      self.fc1 = nn.Linear(768,2).to(device)\n","      self.softmax = nn.LogSoftmax(dim=1).to(device)\n","      return\n","\n","    def forward(self, sent_id, mask):\n","      output_model = self.model(sent_id, attention_mask=mask)\n","      cls_hs = output_model[\"last_hidden_state\"]\n","      x = self.pooler(cls_hs)\n","      x = self.fc1(x)\n","      x = self.relu(x)\n","      x = self.dropout(x)\n","      x = self.softmax(x)\n","      return x\n","\n","\n","class GPT2Model_2FC_Norm(nn.Module):\n","    def __init__(self, model, dropout=0.2):\n","      super(GPT2Model_2FC_Norm, self).__init__()\n","      self.model = model.to(device)\n","      self.pooler = GPT2Pooler()\n","      self.norm_mh = nn.LayerNorm(768).to(device)\n","      self.dropout = nn.Dropout(dropout).to(device)\n","      self.relu =  nn.ReLU().to(device)\n","      self.fc1 = nn.Linear(768,512).to(device)\n","      # self.fc1 = nn.Linear(768,2).to(device)\n","      self.sigmoid = nn.Sigmoid().to(device)\n","      self.fc2 = nn.Linear(512,2).to(device)\n","      self.softmax = nn.LogSoftmax(dim=1).to(device)\n","      self.tanh = nn.Tanh().to(device)      \n","      return\n","\n","    def forward(self, sent_id, mask):\n","      output_model = self.model(sent_id, attention_mask=mask)\n","      cls_hs = output_model[\"last_hidden_state\"]\n","      x = self.pooler(cls_hs)\n","      x = self.norm_mh(x)\n","      x = self.fc1(x)\n","      x = self.relu(x)\n","      x = self.dropout(x)\n","      x = self.fc2(x)\n","      x = self.softmax(x)\n","      return x\n","\n","\n","class GPT2Model_1FC_Norm(nn.Module):\n","    def __init__(self, model, dropout=0.2):\n","      super(GPT2Model_1FC_Norm, self).__init__()\n","      self.model = model.to(device)\n","      self.pooler = GPT2Pooler()\n","      self.norm_mh = nn.LayerNorm(768).to(device)\n","      self.fc1 = nn.Linear(768,2).to(device)\n","      self.relu =  nn.ReLU().to(device)\n","      self.dropout = nn.Dropout(dropout).to(device)\n","      self.softmax = nn.LogSoftmax(dim=1).to(device)\n","      return\n","\n","    def forward(self, sent_id, mask):\n","      output_model = self.model(sent_id, attention_mask=mask)\n","      cls_hs = output_model[\"last_hidden_state\"]\n","      x = self.pooler(cls_hs)\n","      x = self.norm_mh(x)\n","      x = self.fc1(x)\n","      x = self.relu(x)\n","      x = self.dropout(x)\n","      x = self.softmax(x)\n","      return x\n","\n","\n","class GPT2Model_Trans(nn.Module):\n","    def __init__(self, model, dropout=0.2):\n","      super(GPT2Model_Trans, self).__init__()\n","      self.model = model.to(device)\n","      self.pooler2 = GPT2Pooler2()\n","      self.softmax = nn.LogSoftmax(dim=1).to(device)\n","      self.trans_model = TransformerTranslator(\n","            vocab_size, output_size, device, max_length=MAX_LENGHT).to(device)\n","      return\n","\n","    def forward(self, sent_id, mask):\n","      output_model = self.model(sent_id, attention_mask=mask)\n","      cls_hs = output_model[\"last_hidden_state\"]\n","      x = self.trans_model(cls_hs)\n","      x = self.pooler2(x)\n","      x = self.softmax(x)\n","      return x"]},{"cell_type":"markdown","metadata":{"id":"LGFpP494zgLh"},"source":["### Define Train & Evaluate Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BEG81NvL1Rt9"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","from tqdm import tqdm_notebook\n","import random\n","\n","def seed_torch(seed=0):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.manual_seed(seed)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","  \n","seed_torch()\n","\n","\n","def train():\n","  model.train()\n","  total_loss, total_accuracy = 0, 0\n","\n","  for step,batch in enumerate(train_dataloader):\n","    if step % 50 == 0 and not step == 0:\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n","    batch = [r for r in batch]\n","    sent_id, mask, labels = batch\n","    sent_id = sent_id.to(device)\n","    mask = mask.to(device)\n","    labels = labels.to(device)\n","    labels = labels.unsqueeze(1)\n","    optimizer.zero_grad()\n","    preds = model(sent_id, mask)\n","    preds = preds.unsqueeze(2)\n","    # print(preds.shape, labels.shape)\n","    # labels = labels.float()\n","\n","    loss = criterion(preds, labels)\n","    total_loss = total_loss + loss.item()\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","    optimizer.step()\n","    preds=preds.detach().cpu().numpy()\n","\n","  avg_loss = total_loss / len(train_dataloader)\n","  return total_loss, avg_loss\n","\n","\n","def evaluate():\n","  print(\"\\nEvaluating...\")\n","  model.eval()\n","  total_loss, total_accuracy = 0, 0\n","  for step,batch in enumerate(val_dataloader):\n","    if step % 50 == 0 and not step == 0:\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n","    batch = [t for t in batch]\n","    sent_id, mask, labels = batch\n","    sent_id = sent_id.to(device)\n","    mask = mask.to(device)\n","    labels = labels.to(device)\n","    labels = labels.unsqueeze(1)\n","    with torch.no_grad():\n","      preds = model(sent_id, mask)\n","      preds = preds.unsqueeze(2)\n","      # labels = labels.float()\n","      \n","      loss = criterion(preds,labels)\n","      total_loss = total_loss + loss.item()\n","      preds = preds.detach().cpu().numpy()\n","  avg_loss = total_loss / len(val_dataloader)\n","  return avg_loss\n","\n","\n","def find_accuracy(model, dataloader, device, batch_size):\n","    # Set the model to eval mode to avoid weights update\n","    model.eval()\n","\n","    total_accuracy = 0\n","    for step,batch in enumerate(dataloader):\n","      batch = [t for t in batch]\n","      sent_id, mask, labels = batch\n","      sent_id = sent_id.to(device)\n","      mask = mask.to(device)\n","      labels = labels.to(device)\n","      labels = labels.unsqueeze(1)\n","\n","      with torch.no_grad():\n","        preds = model(sent_id, mask)\n","        pred = torch.max(preds, 1)\n","\n","        # translation = preds.mean(1)\n","        # pred = (translation > 0).int()\n","        # labels = labels.float()\n","        \n","        labels = labels.transpose(0, 1)\n","\n","        # print(\"Predictions:\", pred.indices)\n","        # print(\"Labels:\", labels)\n","        # print(pred.indices == labels)\n","        # print(((pred.indices == labels).int()).sum().item())\n","\n","        # acc = ((pred == labels).int()).sum().item()\n","        acc = ((pred.indices == labels).int()).sum().item()\n","        total_accuracy +=  acc\n","    accuracy = total_accuracy / (len(dataloader)*batch_size)\n","    return accuracy"]},{"cell_type":"markdown","metadata":{"id":"IARpecoUzloO"},"source":["### Model training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2623337,"status":"ok","timestamp":1690451680245,"user":{"displayName":"Abanoub Abdelmalak","userId":"09340681942671294066"},"user_tz":-180},"id":"uSN0ZPiW1d-e","outputId":"1b9a647f-3ce7-48a3-b5d5-b619fdcdab30"},"outputs":[],"source":["from transformers import AdamW\n","\n","epochs = 20\n","# criterion = nn.BCEWithLogitsLoss()\n","criterion = nn.NLLLoss()\n","\n","train_losses={}\n","valid_losses={}\n","train_accuracy={}\n","valid_accuracy={}\n","\n","# Logging the outputs\n","run_directory = \"./models/PretrainedModels/GPT2/\"\n","fp = open(run_directory+\"GPT2_ISOT3_run_log.csv\", \"w\")\n","fp.write(\"dropout,lr,train_loss,valid_loss,train_perp,valid_perp,train_acc,valid_acc\\n\")\n","\n","\n","# for dropout in [0.1, 0.2, 0.4]:\n","#     for lr in [1e-5, 1e-4, 1e-3]:\n","\n","for dropout in [0.1]:\n","    for lr in [1e-6]:\n","        model = GPT2Model_1FC(gpt2, dropout=dropout)\n","        # model = GPT2Model_Trans(gpt2, dropout=dropout)\n","        \n","        optimizer = AdamW(model.parameters(), lr = lr)\n","        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n","        key = f'ISOT_GPT2CustomModel_2_lr_{lr}_dropout_{dropout}'\n","        print(\"*\"*50)\n","        print(\"key:\", key)\n","        best_valid_loss = float('inf')\n","        best_train_loss = float('inf')\n","        best_train_acc = float('inf')\n","        best_valid_acc = float('inf')\n","        best_train_perp = float('inf')\n","        best_valid_perp = float('inf')\n","\n","        train_losses[key] = []\n","        valid_losses[key] = []\n","        train_accuracy[key] = []\n","        valid_accuracy[key] = []\n","\n","        for epoch in range(epochs):\n","            print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","            \n","            train_loss, avg_train_loss = train()\n","            scheduler.step(train_loss)\n","            avg_val_loss = evaluate()\n","            acc_train = find_accuracy(model, train_dataloader, device, batch_size)\n","            acc_val = find_accuracy(model, val_dataloader, device, batch_size)\n","\n","            train_perp = np.exp(avg_train_loss)\n","            valid_perp = np.exp(avg_val_loss)\n","\n","            if avg_val_loss < best_valid_loss:\n","                best_valid_loss = avg_val_loss\n","                best_train_loss = avg_train_loss\n","                best_train_acc = acc_train\n","                best_valid_acc = acc_val\n","                best_train_perp = train_perp\n","                best_valid_perp = valid_perp\n","                torch.save(model.state_dict(), f'{key}.pt')\n","\n","            train_losses[key].append(avg_train_loss)\n","            valid_losses[key].append(avg_val_loss)\n","            train_accuracy[key].append(acc_train)\n","            valid_accuracy[key].append(acc_val)\n","\n","            print(f'\\nTraining Loss: {avg_train_loss:.3f}')\n","            print(f'Validation Loss: {avg_val_loss:.3f}')\n","            print(f'Training Accuracy: {acc_train:.3f}')\n","            print(f'Validation Accuracy: {acc_val:.3f}')\n","\n","        fp.write(f'{dropout},{lr},{best_train_loss},{best_valid_loss},{best_train_perp},{best_valid_perp},{best_train_acc},{best_valid_acc}\\n')\n","\n","fp.close()"]},{"cell_type":"markdown","metadata":{"id":"Qp7jw9a6qiPt"},"source":["### Model performance"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1101,"status":"ok","timestamp":1690448443235,"user":{"displayName":"Abanoub Abdelmalak","userId":"09340681942671294066"},"user_tz":-180},"id":"1fVcPQH5oTc6","outputId":"4af3944a-de35-45ed-8a12-788e0f11eb29"},"outputs":[],"source":["# Load weights of best model\n","path = f'{key}.pt'\n","model.load_state_dict(torch.load(path))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":432},"executionInfo":{"elapsed":722,"status":"error","timestamp":1690448454608,"user":{"displayName":"Abanoub Abdelmalak","userId":"09340681942671294066"},"user_tz":-180},"id":"aRgRYquAoWbu","outputId":"7d31cb89-0b8e-4c3d-f5c3-6498e0614d8e"},"outputs":[],"source":["# Calculate testing accuracy and loss\n","with torch.no_grad():\n","  test_seq = test_seq.to(device)\n","  test_mask = test_mask.to(device)\n","\n","  preds = model(test_seq, test_mask)\n","  preds = preds.to(device)\n","  test_y = test_y.to(device)\n","  test_acc = find_accuracy(model, test_dataloader, device, batch_size=batch_size)\n","  test_loss = criterion(preds, test_y)\n","  \n","  preds_np = preds.detach().cpu().numpy()\n","\n","preds_np = np.argmax(preds_np, axis = 1)\n","print(classification_report(test_y, preds_np))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Output the best final results of the GPT2 model\n","best_test_loss = test_loss\n","best_test_ppl = np.exp(test_loss.cpu().detach().numpy())\n","best_test_acc = test_acc\n","\n","print(\"Training Loss, Training Perplexity, Training Accuracy, Validation Loss, Validation Perplexity, Validation Accuracy, Testing Loss, Testing Perplexity, Testing Accuracy\")\n","print(f\"{best_train_loss:.4f}, {np.exp(best_train_loss):.4f}, {best_train_acc:.4f}, {best_valid_loss:.4f}, {np.exp(best_valid_loss):.4f}, {best_valid_acc:.4f}, {best_test_loss:.4f}, {best_test_ppl:.4f}, {best_test_acc:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"jNb-QwxVkn2P","outputId":"d12ed0cd-7656-4036-e151-acb1019be854"},"outputs":[],"source":["# Generate the plots for the GPT2 model\n","plot_name = \"VanillaGPT2_ISOT_TunedWeights_\" + key\n","\n","train_perp = np.exp(train_losses[key])\n","val_perp = np.exp(valid_losses[key])\n","\n","plt.title(\"Loss vs Epoch\")\n","plt.plot(train_losses[key], label=\"Training\")\n","plt.plot(valid_losses[key], label=\"Validation\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.grid(True)\n","plt.legend()\n","plt.savefig(plot_name+\"loss_curve.png\")\n","plt.show()\n","plt.close()\n","\n","plt.title(\"Perplexity vs Epoch\")\n","plt.plot(train_perp, label=\"Training\")\n","plt.plot(val_perp, label=\"Validation\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Perplexity\")\n","plt.grid(True)\n","plt.legend()\n","plt.savefig(plot_name+\"perplexity_curve.png\")\n","plt.show()\n","plt.close()\n","\n","\n","plt.title(\"Accuracy vs Epoch\")\n","plt.plot(train_accuracy[key], label=\"Training\")\n","plt.plot(valid_accuracy[key], label=\"Validation\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.grid(True)\n","plt.legend()\n","plt.savefig(plot_name+\"acc_curve.png\")\n","plt.show()\n","plt.close()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"P95XPCMoq8nN"},"source":["## Fake News Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":466,"status":"ok","timestamp":1690439988197,"user":{"displayName":"Abanoub Abdelmalak","userId":"09340681942671294066"},"user_tz":-180},"id":"vooQxtj5UBt-","outputId":"80a6f7c6-60f3-4141-b56a-928c0ef16c96"},"outputs":[],"source":["# testing on unseen data\n","unseen_news_text_random = [\"A precautionary message that one can catch fire due to hand sanitizer as it has a high amount of alcohol. The message also shows the hands of a lady who after applying sanitizer went near the stove and ended up burning her hands.\",  # Fake\n","                    \"A Pentagon study found that people who get the flu vaccine are 36% more likely to get COVID-19.\",               # Fake\n","                    \"The total number of confirmed cases of COVID-19 is now 1212 which is the number we report to the World Health Organization. There is no one in New Zealand receiving hospital-level care for COVID-19.\",           # True\n","                    \"Our total number of confirmed cases remains at 1205 which is the number we report to the World Health Organization.\"                          # True\n","                    ]\n","\n","# tokenize and encode sequences in the test set\n","MAX_LENGHT = 15\n","tokens_unseen = tokenizer.batch_encode_plus(\n","    unseen_news_text_random,\n","    max_length = MAX_LENGHT,\n","    pad_to_max_length=True,\n","    truncation=True\n",")\n","\n","unseen_seq = torch.tensor(tokens_unseen['input_ids'])\n","unseen_mask = torch.tensor(tokens_unseen['attention_mask'])\n","\n","with torch.no_grad():\n","  unseen_seq = unseen_seq.to(device)\n","  unseen_mask = unseen_mask.to(device)\n","  preds = model(unseen_seq, unseen_mask)\n","  preds = preds.detach().cpu().numpy()\n","\n","print(preds)\n","preds = np.argmax(preds, axis = 1)\n","preds"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
